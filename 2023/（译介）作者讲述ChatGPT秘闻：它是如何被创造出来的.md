---
author: "catbaron"
title: "（译介）作者讲述ChatGPT秘闻：它是如何被创造出来的"
date: "2023-03-14"
draft: false
categories: 
  - 作
tags: 
  - translate
  - chatgpt
  - openai
---

> ChatGPT 爆火之后，网上有各种讨论。这篇文章的作者采访了 OpenAI 的四位相关人员。在谈话中，他们提到了一些有趣的事情。包括他们自己对 ChatGPT 已经相关模型技术的观点。比如 OpenAI 在开发初期就对越狱行为有所预知，他们也知道语言模型会编造一些内容等等。

[原文链接](https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/)

2022年11月，在 OpenAI 这个位于旧金山的AI公司悄无声息地公开了 ChatGPT 的时候，他们并没有报什么期待。然而从那时起他们便一路高歌猛进。

「它当时只是一个内部研究的先行展示」，Sandhini Agarwal，OpenAI 的一个政策人员如此说道。它是作为对一个两年前的技术的重新梳理，而且试图通过收集公众的反馈来修正错误。「我们并没有打算把它作为一个巨大进展来宣传」OpenAI 的一位 ChatGPT 的 Liam Fedus 说。

为了一窥 ChatGPT 背后的故事——它如何被创造，OpenAI 如何在公布之后持续更新，以及它的作者们如何看待它的成功——我和四位曾经参与创造这个互联网上最流行的应用的人聊了一下。除了 Agarwal 和 Fedus，我采访了 John Schulman，OpenAI 的一位联合创始人，以及 Jan Leike。OpenAI 联合团队的领导人。这个团队旨在令 AI 满足用户需求（并不做任何多余之事）。

我能体会到的是，OpenAI 依然对他们实验项目的大获成功感到困惑，但他们抓住了这次机会来继续推进他们的技术发展，通过上百万的用户的使用来发现和修正其中的严重错误。

从 11 月以来，OpenAI 已经对 ChatGPT 进行了数次更新。研究者们通过一种叫做「对抗训练」的技术来组织用户通过欺骗 ChatGPT 来让它做出不好的行为（这种做法被称为越狱）。这项工作的做法是让数个对话机器人互相对抗：一个机器人通过生成一些特定的内容来使得其他机器人突破通常的限制而产生意外的回复。那些成功的攻击则会加入 ChatGPT 的训练数据，让它有望能够忽略这些内容。

OpenAI 和微软签订了一项数十亿美元的协议，并且宣布和一家国际咨询管理公司合作。这家公司计划在包括可口可乐在内的客户的商战中使用 OpenAI 的生成式 AI 模型。此外，ChatGPT 也在全球范围的公司和投资者中引发了针对大规模语言模型的竞争。

短短三个月间已然是满城风雨。ChatGPT究竟从何而来？OpenAI如何是做好公开它的准备的？他们接下来又会如何行事？

*为了行文简洁，下文有修改*

------

**Jan Leike:** 说真的，这太令人震惊了。我们也很意外，想要弄清楚发生了什么事。

**John Schulman:** 在发布之后我一直在刷推特，有段时间上面充满了 ChatGPT 的截图。我预料到它会启发一些人，我也预料到它会收到一些关注，但是我没想到它会在主流人群中如此流行。

**Sandhini Agarwal:** 有如此众多的人使用它，我觉得对我们所有人来说都是个惊喜。我们在这些模型上面工作太久了，以至于忘了他们对外面的人来说会是多么新奇。

**Liam Fedus**: 我们很惊喜地发现它很容易被接受。在此之前已经有很多通用聊天机器人的尝试，我知道我们面临许多挑战。但是我们的内部测试版本给了我们足够的信心，相信我们做的东西会让大家会喜欢的。

**Jan Leike:** 我很想搞清楚是什么在驱使着一切。老实说，我们不懂，我们不知道。

------

_最令这个团队困惑的是，ChatGPT背后的技术并不是什么新东西。在数个月之前，OpenAI公开了一系列大规模语言模型，称为 GPT-3.5，而 ChatGPT 只是一个经过优化的版本。GPT-3.5 本身则是 GPT-3 的更新版本，GTP-3 则诞生于 2020 年。OpenAI 通过网站公开了这些模型的应用编程接口（API），使得其他软件的开发者能够轻松地将这些模型接入自己的代码。在 2022 年 1 月，他们还公开了一个优化版本的 GPT-3.5，即 InstructGPT，但是所有这些事情都没有引起公众的关注。__


**Liam Fedus:** ChatGPT 模型和 InstrcutGPT 是同一个模型的优化版本 ，而且我们用了类似的优化手法。我们只是加入了一些对话数据，调整了一些训练过程。所以我们当时并没有打算把它当作什么大的基础性进展来发布。但它显示出，那些对话数据对 ChatGPT 有不少积极的影响。


**John Schulman:** 本质上来说，ChatGPT 在一些基础测试上并没有表现出明显的变化，但是它有更好的可用性。

**Jan Leike:** 一方面你可以理解为，ChatGPT 作为一个人工智能系统，我们搞出来已经有一段时间了。和我们之前创造出来的模型相比，它在本质上并没有变得更强大。而在 ChatGPT 诞生一年前，同样的基础模型的 API 已经可用。另一方面，我们令它和人类的需求更好地连接起来。它通过对话和你沟通，在聊天的形式下更容易使用，它也会尝试帮助你。这是一个很好的进展，我想这也是大家现在才意识到的事情。

**John Schulman:** 它也更主动的去做一些推测。用户则乐于反复尝试来获得所需。


------

_最令这个团队困惑的是，ChatGPT背后的技术并不是什么新东西。在数个月之前，OpenAI公开了一系列大规模语言模型，称为 GPT-3.5，而 ChatGPT 只是一个经过优化的版本。GPT-3.5 本身则是 GPT-3 的更新版本，GTP-3 则诞生于 2020 年。OpenAI 通过网站公开了这些模型的应用编程接口（API），使得其他软件的开发者能够轻松地将这些模型接入自己的代码。在 2022 年 1 月，他们还公开了一个优化版本的 GPT-3.5，即 InstructGPT，但是所有这些事情都没有引起公众的关注。__
_训练 ChatGPT 的方法和 InstructGPT 非常类似，这是一种叫做人类反馈强化学习（reinforcement learning from human feedback, RLHF）的方法。 这就是 ChatGPT 的秘方所在。基本的思路是，使用一个大型的语言模型（比如 GPT-3.5）来随意生成内容，然后通过告诉它人类倾向于哪些内容来对它进行调整。_


**Jan Leike:** 我们有一个大规模团队来查阅 ChatGPT 的提示语和生成的回应结果，并判断某条回应是不是比另一条更优。所有这些数据最终被收集用于一次训练。我们对 InstructGPT 做的差不多就是类似的事情。你想让它变得有用、真实，你想让它变得——你懂吧——无害。有一些在处理对话协助请求时比较特殊的东西，比如说，如果用户的请求不是很明确，那它应该能够主动询问。同时它应该清楚自己是一个 AI 系统。它不应该给自己设定一个它所没有拥有的身份，也不应该声称自己能做到一些超出它能力范围的事情。当用户要求它去做一些不应为之事，它必须给出拒绝的回应。在这个训练过程中会用到的一个句子是「作为一个OpenAI训练的语言模型……」我们并没有明确要求它使用这句话，但这句话在训练过程中获得了很高的人类评分。

**Sandhini Agarwal:** 没错，我觉得就是这么回事。人类评分员中有很多评分规则，比如真实性之类的。但是他们也开始倾向于他们认为比较得体的回应，比如不要假装自己是其他事物。

_因为 ChatGPT 并没有用到和之前不同的技术，所以 OpenAI 在发布它的时候也没有做任何特殊的准备。他们觉得他们在之前的模型中设置的护栏已经足够充分了。_

**Sandhini Agarwal:**  我们准备发布它的时候，并没有觉得它会带来一个全新的风险。GPT-3.5 已经诞生很久，我们知道它已经足够安全了。而且通过对 ChatGPT 训练，这个模型已经能够自动学习拒绝性回答，并拒绝了很多请求。

**Jan Leike:** 我们为 ChatGPT 组织了一个“红色小组”，这个小组会坐在一起，尝试去破坏这个模型。我们还有一个外部团队在做类似的事情。我们还有一些我们信任的用户，在早期阶段为我们提供反馈。

**Sandhini Agarwal:** 我们确实发现它会生成一些特定的不当内容，但是这些都是 GPT-3.5 也会生成的东西。所以在风险方面，作为一个研究展示——因为这就是它应该有的表现——所以这没什么问题。

**John Schulman:** 你不可能等到它完美无缺再发布。我们已经对早期版本 beta 测试了几个月的时间，测试员对产品的表现印象很好。我们最担心的其实是真实性，因为模型喜欢编造一些事情。不过我们已经有了 InstructGPT 和其他大型语言模型了，所以我们觉得只要 ChatGPT 比他们在真实性和其他问题上更安全，那就应该没什么问题。在发布之前我们通过一些有限的测试确认了这个模型看上去确实比别的模型更加真实和安全，所以我们决定执行发布。

------

_最令这个团队困惑的是，ChatGPT背后的技术并不是什么新东西。在数个月之前，OpenAI公开了一系列大规模语言模型，称为 GPT-3.5，而 ChatGPT 只是一个经过优化的版本。GPT-3.5 本身则是 GPT-3 的更新版本，GTP-3 则诞生于 2020 年。OpenAI 通过网站公开了这些模型的应用编程接口（API），使得其他软件的开发者能够轻松地将这些模型接入自己的代码。在 2022 年 1 月，他们还公开了一个优化版本的 GPT-3.5，即 InstructGPT，但是所有这些事情都没有引起公众的关注。_

_自从 ChatGPT 发布以来，OpenAI一直在观察人们如何使用它，首次目睹了一个大型语言模型在千万用户手中发热过载。这些用户可能在不停地测试它的边界，或者找到它的缺陷。这个团队尝试获取那些 ChatGPT 产生的最糟糕的内容——从[上帝爱恋童癖神父之歌](https://twitter.com/IrvingPeres/status/1599488357499011072) 到窃取信用卡帐号的病毒代码——然用用它们来强化之后的版本。_

**Sandhini Agarwal:** 我们还有许多未竟之事。我们很清楚，ChatGPT 的广泛传播使得很多我们已知的问题变得更加棘手和严重，我们想要尽快解决这些问题。比如，我们知道这个模型存在一些偏见和倾向性。而且，没错，ChatGPT 是很擅长拒绝请求，但是它也很容易被提示语绕过，使它没能拒绝掉一些它应该拒绝的回答。

**Liam Fedus:** 我们看到了 ChatGPT 的各种用法和用户创建的各种应用，这很令人兴奋。但是我们总是会聚焦于那些有待提高的地方。我们认为通过我们的迭代更新，获取反馈，强化模型，我们可以开发出最优最强的技术。随着我们技术的革新，必然会出现新的问题。

**Sandhini Agarwal:** 在发布后的几周内，我们审查了一些用户发现的最糟糕的案例。我们对它们做了一些评估，对如何修正它做了一些讨论。

**Jan Leike:** 有时候是一些已经在 Twitter 迅速传播的案例，但也有一些人会默默提交反馈。

**Sandhini Agarwal:** 我们找到的很多案例都属于「越狱」，这是我们必须要修正的问题。不过这种情况下用户会使用非常复杂的手段来让模型说一些不当的回答，所以它并不是什么完全出乎意料的事情。然而这确实是我们当下正在积极修正的问题。当我们发现了一些越狱行为，我们会把它加入训练数据和测试数据。所有这些我们建立的数据都会用在将来的训练中。

**Jan Leike:**  每次我们有了更好的模型，我们就会把它推出然后测试它。我们乐观地认为，通过有指向型的对抗训练我们能够很好地改善越狱问题。我们不确定越狱问题会不会彻底消失，但是我们觉得我们能够令很多越狱行为变得更加困难。如我们之前所说，我们在发布之前并非没有预料到越狱的可能性。我认为你很能一开发出这样的系统立刻就能预料到它会出现什么安全性问题。所以我们花了很多功夫在监测用户的使用目的，观察会发生什么，然后再做出反应。这不是说我们在预料到可能出现的问题是没有积极的去缓和它，不过，当一个系统刚诞生的时候，你很难预知所有将要发生的事情。

_在一月，微软公布了 Bing Chat，一个[检索机器人](https://www.technologyreview.com/2023/02/16/1068695/chatgpt-chatbot-battle-search-microsoft-bing-google/)_。很多人认为它就是 OpenAI 未正式公开的 GPT-4 模型 [^1]。（OpenAI 说：「Bing 由我们的下一代模型驱动。微软为它针对搜索进行了定制化处理。它融合了 ChatGPT 和 GPT-3.5 的优势。」）科技巨头对聊天机器人的应用，令底层模型的开发者背负了捍卫价值数十亿美元的声音的责任。

**Sandhini Agarwal:** 现在的风险肯定比之前，比如说六个月之前，要高很多。但是它应该还是要比一年后要低。对这些模型来说，它们的应用场景会至关重要。比如对 Google 和 微软来说，一点真实性上的不足也会变得很严重，因为它们将这些模型作为搜索引擎来使用。一个作为搜索引擎的大型语言模型的行为表现，和一个作为消遣的聊天机器人的模型肯定是天差地别的。我们必须清楚地划分这些不同的应用场景，来创造出在不同场景下都有用的东西，尽管它在这些不同场景下会有迥异的表现。这带来了更多的压力。因为我们知道我们建立的模型会转化成产品。 ChatGPT 已经是一个有 API 的产品了。我们正在做一个泛用型的技术，所以我们必须保证它能在任何事情上运转良好。这是我们面临的最大的挑战之一。

**John Schulman**: 我低谷了用户在调查 ChatGPT 行为政策上的热心程度。我们本可以在收集训练数据是有更好的决断，足以缓和这些问题。我们现在正在着手处理它们。

**Jan Leike:** 在我看来，ChatGPT 在很多方面都是失败的——但是没有什么可做之事。我们并没有觉得我们解决了哪些问题。我们必须清楚地告诉自己——以及其他人——这项技术的局限性。我是说，语言模型已经出现了一段时间了，但依然处在早期阶段。它们所有的问题我们都清楚。我认为我们必须超前思考，并且管理预期，然后清晰地告诉大家，这并不是一个已经完成的产品。

[^1]: 现在我们已经知道，彼时的 Bing Chat 并非 GPT-4 模型。GPT-4 模型在本文发表后的三月十四日发布。（译注**）